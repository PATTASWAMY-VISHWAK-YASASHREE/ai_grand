PS-05 Grand Challenge: Advancing Multilingual Document AI
1. What is the fundamental limitation of traditional OCR systems that the PS-05 Grand Challenge aims to overcome?
Traditional Optical Character Recognition (OCR) systems are primarily limited by their inability to accurately extract semantically correct and structured information from complex, visually rich, and multilingual digital documents. They struggle with issues like jumbled or mixed text, difficulties in interpreting mixed scripts, and failing to correctly identify and process non-textual elements such as tables, images, maps, and charts. This results in outputs that are unsuitable for subsequent AI/ML tasks because the visual hierarchy, semantic grouping, and layout fidelity of the original document are lost.
2. What types of structured information must the AI systems accurately extract and preserve from documents?
The AI systems in this challenge must accurately extract structured information while preserving several key aspects. This includes maintaining the visual hierarchy (e.g., distinguishing headings from body text), semantic grouping (e.g., identifying related form fields or captions), and layout fidelity (e.g., retaining table structures, image alignment, and correct reading order). Additionally, the systems must be capable of interpreting embedded elements like charts, plots, maps, and figures, converting their contents into natural language descriptions within the structured output.
3. What are the specific languages targeted for multilingual document parsing in this challenge, and how are non-textual elements handled in the output?
The challenge specifically targets multilingual document parsing across English, Hindi, Urdu, Arabic, Nepalese, and Persian. For non-textual elements such as tables, images, maps, and charts, the output should be a natural language description of their contents in plain text paragraphs. All extracted content, whether text or descriptions of non-textual elements, must also include bounding box coordinates for localization, and for text, line-wise bounding box coordinates along with identified language are required, all formatted in a standardized JSON structure.
4. How are participants' solutions evaluated in Stage 1 of the challenge, both for online submissions and the final offline demonstration?
In Stage 1, participants' solutions are initially evaluated online using a "Mock Dataset" for self-assessment and a leaderboard, and then on a "Shortlisting Dataset" to select finalists. The primary metric for these online evaluations is Mean Average Precision (MaP) at a bounding box threshold of >= 0.5, focusing solely on document layout (detecting Text, Table, Figure, Title, List, and Background). For the final offline evaluation of shortlisted participants, conducted on a "Holdout Test Set" with specific hardware (Ubuntu 24.04 LTS, 48+ core CPU, 256+ GB RAM, A-100 GPU), the assessment expands beyond just the mAP score (50% weight) to include robustness/efficiency (execution time, 10%), resource utilization (memory footprint, 10%), the approach/methodologies used (20%), and the team's capabilities (10%).
5. Why do the evaluation metrics evolve across the three stages of the challenge, and what new metrics are introduced in Stage 2?
The evaluation metrics evolve across the three stages to progressively assess more complex aspects of document understanding, starting with foundational layout detection and moving towards complete semantic interpretation. Stage 1 focuses primarily on "Document Layout" using Mean Average Precision (MaP) to establish basic structural understanding. In Stage 2, the metrics expand significantly to include Character Error Rate (CER) and Word Error Rate (WER) for text extraction accuracy. Furthermore, specialized metrics like BlueRT + BertScore RT are introduced for evaluating the natural language generation from charts and maps, and T2T-Gen for tables, alongside language identification accuracy, precision, and recall. This staged approach allows for a gradual build-up of complexity, ensuring that solutions first master layout and then integrate accurate content extraction and interpretation.
6. What is the expected output format for each document element, and why is a "machine-friendly yet human-readable" format emphasized?
The expected output for each document element is a JSON structure. For tables, images, maps, and charts, the output must be a natural language explanation or description of their contents, accompanied by bounding box coordinates. For plain text, the output should be the extracted text with line-wise bounding box coordinates and identified language. The emphasis on a "machine-friendly yet human-readable" format is crucial because it allows the structured data to be easily processed and leveraged by downstream AI/ML tasks (such as machine translation, Named Entity Recognition, or Retrieval Augmented Generation) while simultaneously remaining comprehensible and verifiable for human review. This dual capability ensures both efficient automated processing and reliable human oversight.
7. What types of input data will participants receive, and what variations must their solutions be able to handle?
Participants will receive input in the form of JPEG/PNG images of documents. Their solutions must be robust enough to handle various image quality challenges, as the datasets may include rotated, blurred, or noisy images. For Stage 1, the ground truth bounding boxes for evaluation are provided in Horizontal Bounding Box (HBB) format and correspond to de-skewed images, implying that solutions might need to perform de-skewing as part of their preprocessing pipeline to align with the evaluation criteria.
8. What is the purpose of the "Mock Dataset" in Stage 1, and how does it differ from the "Shortlisting Dataset"?
The Mock Dataset in Stage 1 serves as a self-assessment tool for participants to test their solutions and track their performance on a leaderboard. While participants submit results on this dataset, they do not have access to the corresponding ground truth, and it is explicitly stated that this dataset is not used for official evaluation or selection of finalists. In contrast, the Shortlisting Dataset is a distinct dataset used for the official evaluation of solutions, with its results directly determining which 15-20 participants will be invited for the final offline solution evaluation. The Mock Dataset is for practice and public visibility, while the Shortlisting Dataset is for competitive selection.
